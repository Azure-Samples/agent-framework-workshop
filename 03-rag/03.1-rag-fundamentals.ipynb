{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Chapter 3.1: RAG Fundamentals\n",
    "\n",
    "> **Your mission**: Build \"Ask HR\" - an intelligent assistant that helps Contoso employees with health plan questions. But first, you need to understand what's happening under the hood.\n",
    "\n",
    "In this notebook, we'll build RAG (Retrieval-Augmented Generation) the hard way first. Why? Because understanding the mechanics will help you debug issues and make better design decisions later.\n",
    "\n",
    "**What you'll build:**\n",
    "1. A search index with your HR documents\n",
    "2. A manual RAG implementation (to feel the pain)\n",
    "3. A function tool that gives your agent search capabilities\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Complete the setup guide (`03.0-setup-guide.md`) first. You need an Azure AI Search service and your `.env` file configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup & Validation\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from workshop_utils import validate_env\n",
    "validate_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Building the Knowledge Base\n",
    "\n",
    "Before Ask HR can answer questions, it needs documents to search. We'll create an Azure AI Search index programmatically - this approach is reproducible, version-controlled, and CI/CD friendly.\n",
    "\n",
    "### Step 1.1: Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Azure AI Search configuration\n",
    "search_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "search_key = os.getenv(\"SEARCH_API_KEY\")\n",
    "index_name = os.getenv(\"INDEX_NAME\")\n",
    "\n",
    "# Azure OpenAI configuration\n",
    "openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "chat_deployment = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "embedding_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "\n",
    "print(f\"Search Endpoint: {search_endpoint}\")\n",
    "print(f\"Index Name: {index_name}\")\n",
    "print(f\"OpenAI Endpoint: {openai_endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-2",
   "metadata": {},
   "source": [
    "### Step 1.2: Define the Index Schema\n",
    "\n",
    "Run this code to define the schema, then we'll explain what each piece does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    VectorSearchProfile,\n",
    "    SemanticConfiguration,\n",
    "    SemanticSearch,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Define the index fields (matching Azure AI Search portal wizard schema)\n",
    "# Field names: chunk_id, parent_id, chunk, title, text_vector\n",
    "fields = [\n",
    "    SearchField(\n",
    "        name=\"chunk_id\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        key=True,\n",
    "        sortable=True,\n",
    "        filterable=True,\n",
    "    ),\n",
    "    SearchField(\n",
    "        name=\"parent_id\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        filterable=True,  # For filtering chunks by source document\n",
    "    ),\n",
    "    SearchField(\n",
    "        name=\"chunk\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "    ),\n",
    "    SearchField(\n",
    "        name=\"title\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "    ),\n",
    "    SearchField(\n",
    "        name=\"text_vector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=3072,  # text-embedding-3-large dimension\n",
    "        vector_search_profile_name=f\"{index_name}-vector-profile\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Configure vector search (matching portal wizard settings)\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=f\"{index_name}-algorithm\",\n",
    "            parameters=HnswParameters(\n",
    "                metric=\"cosine\",\n",
    "                m=4,\n",
    "                ef_construction=400,\n",
    "                ef_search=500,\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=f\"{index_name}-vector-profile\",\n",
    "            algorithm_configuration_name=f\"{index_name}-algorithm\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Configure semantic search\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=f\"{index_name}-semantic-configuration\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        content_fields=[SemanticField(field_name=\"chunk\")],\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "    ),\n",
    ")\n",
    "\n",
    "semantic_search = SemanticSearch(\n",
    "    default_configuration_name=f\"{index_name}-semantic-configuration\",\n",
    "    configurations=[semantic_config],\n",
    ")\n",
    "\n",
    "# Create the index definition\n",
    "index = SearchIndex(\n",
    "    name=index_name,\n",
    "    fields=fields,\n",
    "    vector_search=vector_search,\n",
    "    semantic_search=semantic_search,\n",
    ")\n",
    "\n",
    "print(f\"Index schema defined with {len(fields)} fields\")\n",
    "print(f\"Fields: {[f.name for f in fields]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "schema-explanation",
   "metadata": {},
   "source": [
    "**What you just defined:**\n",
    "- **Fields** define your data structure (like database columns) - we have `chunk_id`, `chunk` (the text), `title`, and `text_vector` (embeddings)\n",
    "- **Vector search config** controls how embeddings are compared (we use cosine similarity with HNSW algorithm)\n",
    "- **Semantic config** enables AI-powered ranking that understands meaning, not just keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-3",
   "metadata": {},
   "source": [
    "### Step 1.3: Create the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index client\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=search_endpoint,\n",
    "    credential=AzureKeyCredential(search_key),\n",
    ")\n",
    "\n",
    "# Create or update the index\n",
    "try:\n",
    "    result = index_client.create_or_update_index(index)\n",
    "    print(f\"âœ… Index '{result.name}' created/updated successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error creating index: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-4",
   "metadata": {},
   "source": [
    "### Step 1.4: Load and Chunk Documents\n",
    "\n",
    "We need to:\n",
    "1. Read the PDF documents from `data/index1/`\n",
    "2. Split them into chunks\n",
    "3. Generate embeddings for each chunk\n",
    "4. Upload to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-documents",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from pathlib import Path\n",
    "import fitz\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text content from a PDF file.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    doc.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 2000, overlap: int = 500) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    \n",
    "    Default settings match the Azure AI Search portal wizard:\n",
    "    - maximumPageLength: 2000 characters\n",
    "    - pageOverlapLength: 500 characters\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        if chunk.strip():  # Only add non-empty chunks\n",
    "            chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac49bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load documents from data/index1/\n",
    "data_path = Path(\"data/index1\")\n",
    "documents = []\n",
    "\n",
    "for pdf_file in data_path.glob(\"*.pdf\"):\n",
    "    print(f\"Processing: {pdf_file.name}\")\n",
    "    text = extract_text_from_pdf(str(pdf_file))\n",
    "    chunks = chunk_text(text)\n",
    "    \n",
    "    # parent_id is the hash of the source document (for grouping chunks)\n",
    "    parent_id = hashlib.md5(pdf_file.name.encode()).hexdigest()\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # chunk_id uniquely identifies each chunk (matches portal wizard format)\n",
    "        chunk_id = hashlib.md5(f\"{pdf_file.name}_{i}\".encode()).hexdigest()\n",
    "        documents.append({\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"parent_id\": parent_id,\n",
    "            \"chunk\": chunk,\n",
    "            \"title\": pdf_file.name,\n",
    "        })\n",
    "\n",
    "print(f\"\\nTotal chunks to index: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chunking-note",
   "metadata": {},
   "source": [
    "> **On Chunk Sizes:** The default chunking (2000 chars, 500 overlap) is a reasonable starting point, but it's not optimal for every use case. Smaller chunks (~500 chars) give more precise matches but may lose context. Larger chunks (~4000 chars) preserve context but may include irrelevant info. There's no universal \"best\" - it depends on your documents and query patterns. Many production systems use 1000-2000 chars with 10-20% overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-5",
   "metadata": {},
   "source": [
    "### Step 1.5: Generate Embeddings\n",
    "\n",
    "We'll use Azure OpenAI to generate vector embeddings for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "# Setup Azure OpenAI client for embeddings (using API key like other notebooks)\n",
    "openai_client = AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=openai_endpoint,\n",
    "    api_key=openai_api_key,\n",
    ")\n",
    "\n",
    "\n",
    "def get_embedding(text: str) -> list[float]:\n",
    "    \"\"\"Generate embedding for a text chunk.\"\"\"\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=text,\n",
    "        model=embedding_deployment\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2014e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all chunks (this may take a few minutes)\n",
    "print(\"Generating embeddings...\")\n",
    "for i, doc in enumerate(documents):\n",
    "    doc[\"text_vector\"] = get_embedding(doc[\"chunk\"])  # Field name matches portal wizard\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Processed {i + 1}/{len(documents)} chunks\")\n",
    "\n",
    "print(f\"âœ… Embeddings generated for {len(documents)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-6",
   "metadata": {},
   "source": [
    "### Step 1.6: Upload Documents to Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload-documents",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Upload documents to the index\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=AzureKeyCredential(search_key)\n",
    ")\n",
    "\n",
    "# Upload in batches (Azure Search has a limit of 1000 documents per batch)\n",
    "batch_size = 100\n",
    "for i in range(0, len(documents), batch_size):\n",
    "    batch = documents[i:i + batch_size]\n",
    "    result = search_client.upload_documents(documents=batch)\n",
    "    succeeded = sum(1 for r in result if r.succeeded)\n",
    "    print(f\"Uploaded batch {i // batch_size + 1}: {succeeded}/{len(batch)} succeeded\")\n",
    "\n",
    "print(f\"\\nâœ… Document upload complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding RAG\n",
    "\n",
    "Now that your knowledge base is ready, let's understand the pattern we'll use to query it.\n",
    "\n",
    "**RAG** (Retrieval-Augmented Generation) is simple: before the LLM answers, we search for relevant documents and include them in the prompt. This grounds responses in facts rather than the model's training data.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        RAG Architecture                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   User Query â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚\n",
    "â”‚                    â”‚                                            â”‚\n",
    "â”‚                    â–¼                                            â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚\n",
    "â”‚         â”‚   Azure AI Search   â”‚  â—„â”€â”€ Vectorized Documents       â”‚\n",
    "â”‚         â”‚   (Retrieval)       â”‚                                 â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚\n",
    "â”‚                   â”‚                                             â”‚\n",
    "â”‚                   â”‚ Retrieved Context                           â”‚\n",
    "â”‚                   â–¼                                             â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚\n",
    "â”‚         â”‚   LLM (Generation)  â”‚  â—„â”€â”€ Query + Context            â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚\n",
    "â”‚                   â”‚                                             â”‚\n",
    "â”‚                   â–¼                                             â”‚\n",
    "â”‚            Grounded Response                                    â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Why does this matter?** RAG solves real problems: it prevents hallucination by grounding answers in facts, keeps knowledge current (just update your index), works within context limits (retrieve only relevant chunks), and keeps proprietary data in your control."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Manual RAG\n",
    "\n",
    "> Now let's build Ask HR the hard way. Spoiler: it's going to be tedious. That's the point - you'll appreciate the abstractions more.\n",
    "\n",
    "We'll implement RAG step by step:\n",
    "1. Search for relevant documents\n",
    "2. Manually inject the results into a prompt\n",
    "3. Generate a response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-1",
   "metadata": {},
   "source": [
    "### Step 3.1: Perform a Raw Search\n",
    "\n",
    "Let's see what the search returns **before** we add any LLM processing. This is the \"Retrieval\" part of RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raw-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import VectorizedQuery\n",
    "\n",
    "\n",
    "def search_documents(query: str, top_k: int = 3) -> list[dict]:\n",
    "    \"\"\"Search the index using hybrid search (text + vector).\"\"\"\n",
    "    # Generate embedding for the query\n",
    "    query_vector = get_embedding(query)\n",
    "    \n",
    "    # Create vector query targeting the text_vector field (matches portal wizard)\n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=query_vector,\n",
    "        k=top_k,\n",
    "        fields=\"text_vector\"  # Field name matches portal wizard schema\n",
    "    )\n",
    "    \n",
    "    # Perform hybrid search\n",
    "    results = search_client.search(\n",
    "        search_text=query,  # Text search on 'chunk' field\n",
    "        vector_queries=[vector_query],\n",
    "        select=[\"chunk_id\", \"parent_id\", \"chunk\", \"title\"],\n",
    "        top=top_k\n",
    "    )\n",
    "    \n",
    "    return [{\n",
    "        \"chunk_id\": r[\"chunk_id\"],\n",
    "        \"title\": r[\"title\"],\n",
    "        \"chunk\": r[\"chunk\"],\n",
    "        \"score\": r[\"@search.score\"]\n",
    "    } for r in results]\n",
    "\n",
    "\n",
    "# Test the search\n",
    "test_results = search_documents(\"What mental health services are covered?\")\n",
    "for r in test_results:\n",
    "    print(f\"ðŸ“„ {r['title']} (score: {r['score']:.3f})\")\n",
    "    print(f\"   {r['chunk']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-2",
   "metadata": {},
   "source": [
    "### Step 3.2: Manual RAG - Injecting Context into the Prompt\n",
    "\n",
    "Now let's manually construct a prompt that includes the retrieved context. This is the core RAG pattern - **retrieve, then generate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-rag",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_framework.azure import AzureOpenAIChatClient\n",
    "\n",
    "# Create chat client using API key (same pattern as other notebooks)\n",
    "chat_client = AzureOpenAIChatClient(\n",
    "    endpoint=openai_endpoint,\n",
    "    api_key=openai_api_key,\n",
    "    api_version=api_version,\n",
    "    deployment_name=chat_deployment,\n",
    ")\n",
    "\n",
    "\n",
    "async def manual_rag(user_question: str) -> str:\n",
    "    \"\"\"\n",
    "    Perform RAG manually:\n",
    "    1. Search for relevant documents\n",
    "    2. Build a prompt with the context\n",
    "    3. Generate a response\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant context\n",
    "    retrieved_docs = search_documents(user_question, top_k=3)\n",
    "    \n",
    "    # Step 2: Format context for the prompt\n",
    "    context_text = \"\\n\\n\".join([\n",
    "        f\"[Source: {doc['title']}]\\n{doc['chunk']}\" \n",
    "        for doc in retrieved_docs\n",
    "    ])\n",
    "    \n",
    "    # Step 3: Create agent with static instructions\n",
    "    agent = chat_client.as_agent(\n",
    "        name=\"manual_rag_agent\",\n",
    "        instructions=\"\"\"You are an HR assistant for Contoso Electronics.\n",
    "Answer questions based on the provided context.\n",
    "If the context doesn't contain the answer, say \"I don't have that information.\"\n",
    "\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Step 4: Combine context with user question in the message\n",
    "    augmented_message = f\"\"\"## Retrieved Context:\n",
    "{context_text}\n",
    "\n",
    "## Question:\n",
    "{user_question}\"\"\"\n",
    "    # Step 5: Generate response using the agent\n",
    "    response = await agent.run(augmented_message)\n",
    "    return response.text\n",
    "\n",
    "\n",
    "# Test manual RAG\n",
    "question = \"Do I have to pay anything for my annual checkup?\"\n",
    "answer = await manual_rag(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-rag-problems",
   "metadata": {},
   "source": [
    "### The Problems with Manual RAG\n",
    "\n",
    "That worked, but notice what you had to do: write search logic, format the context, build the prompt, handle the response. Now imagine doing this for every agent, handling multi-turn conversations (where you need to track what was already retrieved), managing token limits when context gets too long, and combining multiple knowledge sources. It's a lot of boilerplate.\n",
    "\n",
    "In Part 4, we'll improve this with function tools. In the next notebook, Context Providers will solve these problems elegantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise1-filtering",
   "metadata": {},
   "source": [
    "### Exercise 1: Add Metadata Filtering\n",
    "\n",
    "Our search returns results from all documents. What if you only want results from a specific PDF?\n",
    "\n",
    "**Your task:** Extend `search_documents()` to accept an optional `source_filter` parameter that filters by document title.\n",
    "\n",
    "**Hints:**\n",
    "- Use the `filter` parameter in `search_client.search()`\n",
    "- OData filter syntax: `\"title eq 'Your Document Title.pdf'\"`\n",
    "- Make sure to handle the case where `source_filter` is `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise1-workspace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents_filtered(\n",
    "    query: str, \n",
    "    source_filter: str = None, \n",
    "    top_k: int = 3\n",
    ") -> list[dict]:\n",
    "    \"\"\"Search documents with optional source filtering.\"\"\"\n",
    "    query_vector = get_embedding(query)\n",
    "    vector_query = VectorizedQuery(vector=query_vector, k=top_k, fields=\"text_vector\")\n",
    "    \n",
    "    # TODO: Build filter expression if source_filter is provided\n",
    "    filter_expr = None  # Replace this line\n",
    "    \n",
    "    results = search_client.search(\n",
    "        search_text=query,\n",
    "        vector_queries=[vector_query],\n",
    "        filter=filter_expr,\n",
    "        select=[\"chunk_id\", \"parent_id\", \"chunk\", \"title\"],\n",
    "        top=top_k\n",
    "    )\n",
    "    return [{\"title\": r[\"title\"], \"chunk\": r[\"chunk\"]} for r in results]\n",
    "\n",
    "\n",
    "# Test without filter (should return results from any document)\n",
    "print(\"=== Without filter ===\")\n",
    "results = search_documents_filtered(\"deductible\")\n",
    "for r in results:\n",
    "    print(f\"  - {r['title']}\")\n",
    "\n",
    "# Test with filter (should only return results from the specified document)\n",
    "print(\"\\n=== With filter (Northwind_Health_Plus only) ===\")\n",
    "results = search_documents_filtered(\n",
    "    \"deductible\", \n",
    "    source_filter=\"Northwind_Health_Plus_Benefits_Details.pdf\"\n",
    ")\n",
    "for r in results:\n",
    "    print(f\"  - {r['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise1-wrapup",
   "metadata": {},
   "source": [
    "When your filter is working correctly, the second test should only return results from `Northwind_Health_Plus_Benefits_Details.pdf`. This pattern is useful when users want to scope their questions to specific documents.\n",
    "\n",
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "def search_documents_filtered(\n",
    "    query: str, \n",
    "    source_filter: str = None, \n",
    "    top_k: int = 3\n",
    ") -> list[dict]:\n",
    "    \"\"\"Search documents with optional source filtering.\"\"\"\n",
    "    query_vector = get_embedding(query)\n",
    "    vector_query = VectorizedQuery(vector=query_vector, k=top_k, fields=\"text_vector\")\n",
    "    \n",
    "    # Build filter if source specified\n",
    "    filter_expr = f\"title eq '{source_filter}'\" if source_filter else None\n",
    "    \n",
    "    results = search_client.search(\n",
    "        search_text=query,\n",
    "        vector_queries=[vector_query],\n",
    "        filter=filter_expr,\n",
    "        select=[\"chunk_id\", \"parent_id\", \"chunk\", \"title\"],\n",
    "        top=top_k\n",
    "    )\n",
    "    return [{\"title\": r[\"title\"], \"chunk\": r[\"chunk\"]} for r in results]\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Function Tools for Search\n",
    "\n",
    "> What if the agent could decide *when* to search, rather than us searching on every query?\n",
    "\n",
    "Function tools give agents agency. Instead of always retrieving context, the agent calls a search tool when it needs information. Use this approach when retrieval is optional (e.g., a general assistant that sometimes needs to look things up). Use Context Providers (next notebook) when every query needs grounding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "search-tool",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "import json\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "\n",
    "class AzureAISearchTool:\n",
    "    \"\"\"\n",
    "    A function tool that wraps Azure AI Search for use by an agent.\n",
    "    \n",
    "    The agent decides when to call this tool based on the user's question.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, search_client: SearchClient, index_name: str):\n",
    "        self.search_client = search_client\n",
    "        self.index_name = index_name\n",
    "    \n",
    "    def search(\n",
    "        self, \n",
    "        query: Annotated[str, \"The search query to find relevant documents\"],\n",
    "        top: Annotated[int, \"Number of results to return\"] = 5\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Search the Contoso HR knowledge base for information about health plans,\n",
    "        policies, and employee benefits.\n",
    "        \n",
    "        Use this tool to find:\n",
    "        - Health plan details (Northwind Health Plus, Northwind Standard)\n",
    "        - Coverage information, deductibles, and copays\n",
    "        - Eligibility requirements and enrollment procedures\n",
    "        - Company HR policies and benefits\n",
    "        \"\"\"\n",
    "        try:\n",
    "            results = self.search_client.search(\n",
    "                search_text=query,\n",
    "                top=top,\n",
    "                query_type=\"semantic\",\n",
    "                semantic_configuration_name=f\"{self.index_name}-semantic-configuration\",\n",
    "            )\n",
    "            \n",
    "            docs = []\n",
    "            for doc in results:\n",
    "                docs.append({\n",
    "                    \"content\": doc.get(\"chunk\", \"\"),\n",
    "                    \"score\": doc.get(\"@search.score\", 0),\n",
    "                    \"source\": doc.get(\"title\", \"Unknown\"),\n",
    "                })\n",
    "            \n",
    "            return json.dumps(docs, indent=2)\n",
    "            \n",
    "        except HttpResponseError as e:\n",
    "            return json.dumps({\"error\": str(e)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tool-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the search tool\n",
    "search_tool = AzureAISearchTool(search_client=search_client, index_name=index_name)\n",
    "\n",
    "# Create an agent with the search tool\n",
    "tool_agent = chat_client.as_agent(\n",
    "    name=\"hr_assistant\",\n",
    "    instructions=\"\"\"\n",
    "    You are an HR Assistant for Contoso Electronics specializing in employee health plans.\n",
    "    \n",
    "    IMPORTANT RULES:\n",
    "    1. ALWAYS use the search tool before answering questions about health plans\n",
    "    2. Base your responses ONLY on information from the search results\n",
    "    3. If the search returns no relevant information, say so explicitly\n",
    "    4. Cite which plan (Northwind Health Plus or Standard) the information comes from\n",
    "    \"\"\",\n",
    "    tools=[search_tool.search]\n",
    ")\n",
    "\n",
    "# Test the tool-based agent\n",
    "response = await tool_agent.run(\"What are the out-of-network copays?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tool-agent-test2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a question that spans multiple topics\n",
    "response = await tool_agent.run(\n",
    "    \"Compare the deductibles between Northwind Health Plus and Northwind Standard\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise2-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 2: Multi-Query Search for Better Recall\n",
    "\n",
    "A single search query might miss relevant documents due to vocabulary mismatch. For example, searching \"cost\" might miss documents that say \"price\" or \"fee\". \n",
    "\n",
    "**Multi-query search** improves recall by:\n",
    "1. Generating multiple query variations (synonyms, rephrasings)\n",
    "2. Running each query against the index\n",
    "3. Combining and deduplicating results\n",
    "\n",
    "This is a real production technique - many RAG systems use an LLM to generate query variations before searching.\n",
    "\n",
    "**Your task:** Implement `multi_query_search()` that takes a list of query variations and returns deduplicated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise2-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's see the problem - different phrasings return different results\n",
    "queries = [\n",
    "    \"What is the cost of the health plan?\",\n",
    "    \"How much do I pay for coverage?\",\n",
    "    \"What are the premium fees?\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    results = search_documents(q, top_k=2)\n",
    "    print(f\"Query: {q}\")\n",
    "    for r in results:\n",
    "        print(f\"  - {r['chunk'][:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise2-workspace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_query_search(queries: list[str], top_k: int = 3) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Search with multiple query variations and combine results.\n",
    "    \n",
    "    Args:\n",
    "        queries: List of query variations (e.g., synonyms, rephrasings)\n",
    "        top_k: Number of results per query\n",
    "    \n",
    "    Returns:\n",
    "        Deduplicated list of results, keeping highest score for duplicates\n",
    "    \"\"\"\n",
    "    # TODO: \n",
    "    # 1. Run search_documents() for each query\n",
    "    # 2. Combine all results\n",
    "    # 3. Deduplicate by chunk content (keep the one with highest score)\n",
    "    # Hint: Use a dict with chunk text as key to deduplicate\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Your implementation here\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Test: these queries should find more relevant docs together than any single query\n",
    "query_variations = [\n",
    "    \"What is the deductible?\",\n",
    "    \"How much do I pay before insurance kicks in?\",\n",
    "    \"Out of pocket amount before coverage\"\n",
    "]\n",
    "\n",
    "combined_results = multi_query_search(query_variations, top_k=2)\n",
    "print(f\"Found {len(combined_results)} unique results from {len(query_variations)} queries\")\n",
    "for r in combined_results[:5]:  # Show top 5\n",
    "    print(f\"  - [{r.get('score', 0):.3f}] {r['chunk'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise2-wrapup",
   "metadata": {},
   "source": [
    "With multi-query search, you should get more comprehensive results than any single query alone. In production, you'd use an LLM to generate these query variations automatically based on the user's question.\n",
    "\n",
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "def multi_query_search(queries: list[str], top_k: int = 3) -> list[dict]:\n",
    "    \"\"\"Search with multiple query variations and combine results.\"\"\"\n",
    "    seen = {}  # chunk -> result with highest score\n",
    "    \n",
    "    for query in queries:\n",
    "        results = search_documents(query, top_k=top_k)\n",
    "        for r in results:\n",
    "            chunk = r['chunk']\n",
    "            # Keep the result with the highest score\n",
    "            if chunk not in seen or r.get('score', 0) > seen[chunk].get('score', 0):\n",
    "                seen[chunk] = r\n",
    "    \n",
    "    # Return sorted by score descending\n",
    "    return sorted(seen.values(), key=lambda x: x.get('score', 0), reverse=True)\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Bonus Exercise:** Wrap `multi_query_search` as a tool and create an agent that generates query variations from a single user question. The agent would take a question like \"What's covered for dental?\" and automatically generate variations like \"dental coverage\", \"teeth benefits\", \"orthodontic services\" before searching. This is how production RAG systems improve recall without requiring users to think of synonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "You just built Ask HR the hard way - and felt the pain. You:\n",
    "\n",
    "1. Created a search index programmatically (reproducible, version-controlled)\n",
    "2. Built manual RAG and saw how much boilerplate it requires\n",
    "3. Wrapped search as a function tool to give agents control over when to retrieve\n",
    "\n",
    "**The key insight:** Manual RAG works, but it's tedious. You're responsible for search logic, prompt construction, multi-turn state, and token management - for every agent.\n",
    "\n",
    "### Production Considerations: Blob Storage Integration\n",
    "\n",
    "In this notebook, we indexed documents programmatically from local files. This approach is great for learning and development, but in production scenarios you'll want a more streamlined pipeline.\n",
    "\n",
    "**The recommended production approach** is to connect Azure Blob Storage directly to Azure AI Search using indexers. This enables:\n",
    "- **Automatic indexing** - Documents uploaded to Blob Storage are automatically indexed\n",
    "- **Incremental updates** - Only new or changed documents are re-indexed\n",
    "- **Managed identity authentication** - Secure, keyless connections between services\n",
    "- **Built-in skillsets** - Azure AI Search can automatically chunk, embed, and enrich documents\n",
    "\n",
    "For a complete guide on setting up this production-ready architecture with Blob Storage, managed identities, and automated indexing pipelines, see **[guides/blob-storage-ai-search-integration.md](../guides/blob-storage-ai-search-integration.md)**.\n",
    "\n",
    "### Next: Context Providers\n",
    "\n",
    "In **03.2**, you'll see how Context Providers eliminate this boilerplate. They automatically inject relevant context before every LLM call, handle multi-turn conversations, and make combining multiple knowledge sources trivial. Ask HR is about to get a lot easier to build."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
